{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4516c3e",
   "metadata": {},
   "source": [
    "# Detección de movimientos fraudulentos\n",
    "##  Análisis de  284.807 Tárjetas de Crédito\n",
    "\n",
    "El siguiente caso de estudio es un conjunto de 284.807 entradas de movimientos en las tarjetas de crédito. En estos datos existe un pequeño porcentaje que son movimiento fraudulentos de modo que me dispongo a analizarlo utilizando algoritmos como **Xgboost, Red Neuronal Artificial, Teorema de Bayes, Regresión Logística, Soporte Vectorial, etc.**\n",
    "\n",
    "link data: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "\n",
    "**Quiero destacar que todos estos datos están pseudonimizados, menos 3 columnas necesarias (de las 31) que seria el tiempo, el importe del movimiento y el tipo de transacción que se realiza (1 si es fraude y 0 si no lo es)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8915aa",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e831443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bd9a7",
   "metadata": {},
   "source": [
    "### Clase Algoritmos\n",
    "Esta clase se encargará de calcular la precisión de los algoritmos siempre y cuando inicialice correctamente el constructor con sus parametros correctos. Después solo hay que llamar a la funcion **precision()**. También almacenará los datos de precisión en un dataframe **df** para una consulta posterior en el apartado de la conclusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bd33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "datos = {'Nombre_Algoritmo':[], 'Precision':[]}  \n",
    "df = pd.DataFrame(data=datos) \n",
    "# -------------------------------------------------------------------------------------\n",
    "class Algoritmos:\n",
    "    \n",
    "    def __init__(self,nombre,cm, y_test):\n",
    "        self.nombre = nombre\n",
    "        self.cm = cm\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (self.nombre)\n",
    "        \n",
    "    def precision(self):\n",
    "        global df\n",
    "        assert(self.cm.shape==(2,2))\n",
    "        assert(type(self.y_test) == pd.DataFrame)\n",
    "        assert(type(self.cm[0][0]) == np.int64)\n",
    "        precision_02 = ((self.cm[0][0] + self.cm[1][1])/len(self.y_test)) * 100\n",
    "        assert(type(precision_02) == np.float64)\n",
    "        df = df.append({'Nombre_Algoritmo':self.nombre, 'Precision':precision_02}, ignore_index=True)\n",
    "        return(\"%.2f\" %precision_02)\n",
    "        \n",
    "#-----------------------------------------------------------------------------------        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af780e19",
   "metadata": {},
   "source": [
    "###  Carga y Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92614bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH\n",
    "path = \"C:/Users/USUARIO/Desktop/DataTarjetaCredit/\"\n",
    "data = pd.read_csv(path + \"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b646c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727ec0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizo cuantos movimimientos fraudulentos hay. \n",
    "p = data['Class'] == 1\n",
    "filtro = data[p]\n",
    "len(filtro.index) # 492 fraudulentos de 284.807 movimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec993b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe() \n",
    "# Si compruebo la media y las desviaciones se observa a simple vista que hay \n",
    "# una alta desviación, por ejemplo en la columna \"Amount (importe) con un \n",
    "# max de '25691.16000' cuando su media es de 88 y la desviacion del 250.\n",
    "# También existe una gran desviación en los valores V1-V28 pero como son\n",
    "# valores pseudonimizados hasta ahí puedo observar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afcdb5",
   "metadata": {},
   "source": [
    "### Preprocesado de datos\n",
    "\n",
    "Primero compruebo si hay algun valor nulo en mi conjunto de datos. Luego observo el tipo de datos por si existiera alguna columna tipo object para transformarla. En este caso no es necesario ya que todas son entradas de datos tipo float64 y una int64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe25bdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesado\n",
    "data.isnull().sum() # ningun dato nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a42346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes # Tipo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c5378",
   "metadata": {},
   "source": [
    "### Transformacíon de datos\n",
    "\n",
    "Almaceno las columnas que considero variables independientes y dependiente, por ejemplo en este supuesto todas las columnas son importantes en el problema de modo que no me deshago de ninguna. El siguiente paso es considerar quien es la variable dependiente y las variables independientes:\n",
    "\n",
    "- El objetivo de este supesto es predecir la columna **'Class'** para pronosticar si un movimiento es fraudulento  o no . De modo que la columna **'Class' será la variable dependiente**.\n",
    "\n",
    "- Como he dicho antes, todas las columnas tienen relevancia en este problema entonces **las variables independientes seran todas las columnas menos 'Class'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0054e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:-1]\n",
    "Y = data.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cd3eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fc12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "...       ...\n",
       "284802      0\n",
       "284803      0\n",
       "284804      0\n",
       "284805      0\n",
       "284806      0\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf81ae",
   "metadata": {},
   "source": [
    "### TRAIN Y TEST\n",
    "Después de visualizar los datos, preprocesarlos, considerar que columnas son relevantes para el caso de estudio y dividirlos en dos matrices ahora ya se pasaría a la fase de división de datos, es decir, considerar cuanto porcentaje de datos del dataset sería para entrenar el modelo, otro porcentaje para testearlo y verificar su precisión. Normalmente:\n",
    "\n",
    "- **TRAIN**, tiene el **80% de datos del dataset original** ya que cuanto más datos más comprobaciones realizará y más fiable será.\n",
    "- **TEST**, tiene el **20% de datos del dataset original** y con él se mide la precisión.\n",
    "\n",
    "Como sería un proceso muy laborioso hacerlo manualmente con está gran cantidad de datos hay una función en Python del modulo **sklearn** llamada **train_test_split** que nos facilita esta división de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a264c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido los datos usando la funcion train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=(0))\n",
    "\n",
    "# Escalado de variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Train tiene 227.845 datos para entrenar\n",
    "# Test tiene 56.926 datos para testear nuestros algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3ecb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = y_test['Class'] == 1\n",
    "filtro = y_test[p]\n",
    "len(filtro.index) # Hay un total de 101 movimientos fraudulentos en los datos para testear el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe5044",
   "metadata": {},
   "source": [
    "### Algoritmo Regresión Logística\n",
    "La regresión logística es un tipo de análisis de regresión utilizado para predecir el resultado de una variable categórica (una variable que puede adoptar un número limitado de categorías) en función de las variables independientes o predictoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4472ea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56852,     9],\n",
       "       [   37,    64]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticR = LogisticRegression(random_state=0) # random state = 0\n",
    "LogisticR.fit(X_train, y_train) #Ajustamos el modelo\n",
    "\n",
    "# Realizamos la predicción de los resultados con el conjunto de testing\n",
    "y_pred = LogisticR.predict(X_test)\n",
    "\n",
    "# Matriz de confusion\n",
    "# De los 56.926 datos  entradas de clientes\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ec450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1] # Tiene una precisión del 99,92%, ha acertado 64 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be92540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.92'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reg = Algoritmos(\"Regresión Logística\", cm, y_test)\n",
    "Reg.precision() # Precisión del 99.92%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1ef07",
   "metadata": {},
   "source": [
    "### Algoritmo KNN (K Vecinos más cercanos)\n",
    "\n",
    "Este algoritmo es un clasificador de aprendizaje supervisado no paramétrico, que utiliza la proximidad para hacer clasificaciones o predicciones sobre la agrupación de un punto de datos individual. Si bien se puede usar para problemas de regresión o clasificación, generalmente se usa como un algoritmo de clasificación, partiendo de la suposición de que se pueden encontrar puntos similares cerca uno del otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ee3efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56852,     9],\n",
       "       [   20,    81]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # Objeto clasifier\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "y_pred = KNN.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) #  99.95% de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aecde6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1] # Tiene una precisión del 99.95%, ha acertado 81 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577aa4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.95'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = Algoritmos(\"KNV\", cm, y_test) \n",
    "KNN.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d267f",
   "metadata": {},
   "source": [
    "### Algoritmo Teorema de Bayes\n",
    "El teorema de Bayes es utilizado para calcular la probabilidad de un suceso, teniendo información de antemano sobre ese suceso. Podemos calcular la probabilidad de un suceso A, sabiendo además que ese A cumple cierta característica que condiciona su probabilidad. El teorema de Bayes entiende la probabilidad de forma inversa al teorema de la probabilidad total. El teorema de la probabilidad total hace inferencia sobre un suceso B, a partir de los resultados de los sucesos A. Por su parte, Bayes calcula la probabilidad de A condicionado a B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d94c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55636,  1225],\n",
       "       [   15,    86]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAYES = GaussianNB()\n",
    "BAYES.fit(X_train, y_train)\n",
    "\n",
    "y_pred = BAYES.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) #97.82 % de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1456e9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]  # Tiene una precisión del 97.82%, ha acertado 86 de 101 en transacciones fraudulentas del testeo de 56.926 datos\n",
    "# pero tiene demasiados falsos positivos 1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "054eef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97.82'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeorBayes = Algoritmos(\"Teorema de Bayes\", cm, y_test)\n",
    "TeorBayes.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80205a3",
   "metadata": {},
   "source": [
    "### Árbol de Clasificación\n",
    "Un árbol de clasificación es un tipo de árbol de decisiones. Utiliza la medida de impurezas de Gini para clasificar los registros en las categorías del campo objetivo. Las predicciones se basan en combinaciones de valores en los campos de entrada.\n",
    "Un árbol de clasificación calcula la categoría de destino pronosticada para cada nodo en el árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "884e0b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56836,    25],\n",
       "       [   23,    78]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DecisionTree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = DecisionTree.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)  # 99,92% de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bd92688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1] # Tiene una precisión del 99,92%, ha acertado 78 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d4c1790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.92'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arbol = Algoritmos(\"Árboles de Clasificación\", cm, y_test)\n",
    "Arbol.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddf12809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56854,     7],\n",
       "       [   24,    77]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aleatorio Entropia\n",
    "rand = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "rand.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rand.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # 99.95% precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90be0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]  # Tiene una precisión del 99.95%, ha acertado 77 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5db42d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.95'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArbolRandomEntropia = Algoritmos(\"Árboles Aleatorios Entropia\", cm, y_test)\n",
    "ArbolRandomEntropia.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f47616e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56855,     6],\n",
       "       [   26,    75]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aleatorio Gini\n",
    "rand02 = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 0)\n",
    "rand02.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rand02.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # 99.94%% precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7565a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]  # Tiene una precisión del 99.94%, ha acertado 75 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6970fb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.94'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArbolRandomGini = Algoritmos(\"Árboles Aleatorios Gini\", cm, y_test)\n",
    "ArbolRandomGini.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db142b2",
   "metadata": {},
   "source": [
    "### Algoritmo de XGBOOST\n",
    "XGBoost es una biblioteca de aumento de gradiente distribuida optimizada diseñada para ser altamente eficiente , flexible y portátil . Implementa algoritmos de aprendizaje automático bajo el marco Gradient Boosting . XGBoost proporciona un impulso de árbol paralelo (también conocido como GBDT, GBM) que resuelve muchos problemas de ciencia de datos de una manera rápida y precisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e69c5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56855,     6],\n",
       "       [   17,    84]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBOOST = XGBClassifier()\n",
    "XGBOOST.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGBOOST.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # % precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b91f922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1] # Tiene una precisión del 99.96%, ha acertado 84 de 101 en transacciones fraudulentas del testeo de 56.926 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8383ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.96'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = Algoritmos(\"XGBOOST\", cm, y_test)\n",
    "xgb.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175b4bd",
   "metadata": {},
   "source": [
    "### Algoritmo Red Neuronal Artificial\n",
    "Una red neuronal es un modelo simplificado que emula el modo en que el cerebro humano procesa la información: Funciona simultaneando un número elevado de unidades de procesamiento interconectadas que parecen versiones abstractas de neuronas.\n",
    "La red aprende examinando los registros individuales, generando una predicción para cada registro y realizando ajustes a las ponderaciones cuando realiza una predicción incorrecta. Este proceso se repite muchas veces y la red sigue mejorando sus predicciones hasta haber alcanzado uno o varios criterios de parada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b6573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0219 - accuracy: 0.9984\n",
      "Epoch 2/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 3/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 4/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 5/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 6/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 7/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 8/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 9/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 10/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 11/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 12/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 13/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 14/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 15/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 16/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 17/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 18/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 19/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 20/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 21/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 22/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 23/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 24/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 25/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 26/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 27/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 28/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 29/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 30/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 31/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 32/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 33/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 34/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 35/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 36/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 37/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 38/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 39/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 40/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 41/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 42/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 43/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 44/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 45/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 46/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 47/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 48/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 49/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 50/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 51/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 52/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 53/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 54/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 55/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 56/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 57/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 59/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 60/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 61/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 62/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 63/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 64/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 65/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 66/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 67/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 68/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 69/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 70/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 71/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 72/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 73/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 74/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 75/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 76/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 77/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 78/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 79/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 80/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 81/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 82/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 83/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 84/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 85/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 86/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 87/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 88/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 89/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 90/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 91/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 92/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 93/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 94/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 95/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 96/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 97/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 98/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 99/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 100/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 101/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 102/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 103/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 104/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 105/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 106/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 107/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 108/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 109/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 110/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 111/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 112/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 113/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 114/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 115/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 116/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 117/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 118/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 119/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 120/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 121/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 122/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 123/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 124/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 125/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 126/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 127/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 128/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 129/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 130/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 131/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 132/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 133/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 134/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 135/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 136/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 137/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 138/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 139/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 140/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 141/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 142/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 143/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 144/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 145/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 146/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 147/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 148/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 149/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 150/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 151/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 152/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 153/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 154/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 155/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 156/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 158/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 159/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 160/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 161/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 162/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 163/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 164/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 165/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 166/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 167/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 168/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 169/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 170/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 171/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 172/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 173/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 174/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 175/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 176/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 177/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 178/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 179/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 180/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 181/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 182/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 183/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 184/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 185/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 186/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 187/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 188/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 189/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 190/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 191/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 192/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 9.7485e-04 - accuracy: 0.9997\n",
      "Epoch 193/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 194/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 9.7918e-04 - accuracy: 0.9997\n",
      "Epoch 195/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 9.9429e-04 - accuracy: 0.9997\n",
      "Epoch 196/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 197/200\n",
      "2279/2279 [==============================] - 6s 3ms/step - loss: 9.8971e-04 - accuracy: 0.9998\n",
      "Epoch 198/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 199/200\n",
      "2279/2279 [==============================] - 5s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 200/200\n",
      "2279/2279 [==============================] - 6s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aec958f3c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir la red neuronal artificial con keras del modelo secuencial\n",
    "RNA = tf.keras.models.Sequential()\n",
    "\n",
    "# 1º Capa de inputs\n",
    "RNA.add(tf.keras.layers.Dense(units=10, activation='relu'))\n",
    "# 2º Capa oculta de 10 nodos\n",
    "RNA.add(tf.keras.layers.Dense(units=10, activation='relu'))\n",
    "# Ultima capa de salidad, con funcion sigmoide\n",
    "RNA.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "RNA.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "RNA.fit(X_train, y_train, batch_size = 100, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b7eca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56852,     9],\n",
       "       [   24,    77]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = RNA.predict(X_test)\n",
    "y_pred_02 = (y_pred > 0.5)\n",
    "y_pred_02 = np.where(y_pred_02=='True',1,y_pred_02)\n",
    "cm = confusion_matrix(y_test, y_pred_02)    # \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "271ad58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e94e4898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.94'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redneuro = Algoritmos(\"Red Neuronal Artificial\", cm, y_test)\n",
    "redneuro.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b29e2",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210587c",
   "metadata": {},
   "source": [
    "Estas son las precisiones que he sacado de mis algoritmo después de testearlos con datos que no conocían. Aunque son precisiones muy parecidas, **la mejor es el algorimto XGBOOST con 99.96% de precisión.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "954f68e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre_Algoritmo</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>99.959622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNV</td>\n",
       "      <td>99.949089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Árboles Aleatorios Entropia</td>\n",
       "      <td>99.945578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Árboles Aleatorios Gini</td>\n",
       "      <td>99.943822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Red Neuronal Artificial</td>\n",
       "      <td>99.942067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regresión Logística</td>\n",
       "      <td>99.919244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Árboles de Clasificación</td>\n",
       "      <td>99.915733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teorema de Bayes</td>\n",
       "      <td>97.823110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Nombre_Algoritmo  Precision\n",
       "6                      XGBOOST  99.959622\n",
       "1                          KNV  99.949089\n",
       "4  Árboles Aleatorios Entropia  99.945578\n",
       "5      Árboles Aleatorios Gini  99.943822\n",
       "7      Red Neuronal Artificial  99.942067\n",
       "0          Regresión Logística  99.919244\n",
       "3     Árboles de Clasificación  99.915733\n",
       "2             Teorema de Bayes  97.823110"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.sort_values(by='Precision',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dedcf",
   "metadata": {},
   "source": [
    "El algoritmo de **Xgboost** tiene la siguiente matriz de confusión:\n",
    "- **array([[56855,     6],[   17,    84]],)**, que se traduciría a que ha detectado **84 movimientos fraudulentos de 101**, con **precisión 99.959622 %**.\n",
    "\n",
    "Por otro lado, el **teorema de Bayes** ha sacado más movimientos fraudulentos que este algoritmo anterior, a continuación su matriz de confusión:\n",
    "\n",
    "- **array([[55636,  1225],[   15,    86]],)** que se traduciría a que ha detectado **86 movimientos fraudulentos de 101**, con **precisión 97.823110 %**.\n",
    "\n",
    "Entonces, **¿cúal es mejor?**. Obviamente uno pensaría que el Teorema de Bayes es mejor que Xgboost porque \"acierta\" más transacciones fraudulentas que Xgboost pero también tiene muchos más falsos positivos-negativos que Xgboost.\n",
    "\n",
    "**A nivel de eficacia Xgboost solo se equivoca en 23 datos de 56.926 del total de transacciones mientras que Bayes se equivoca en 1240 datos, por tanto el mejor algoritmo para pronosticar/analizar nuevos datos de transacciones fraudulentas con estás características sería Xgboost con 99.96% de precisión.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85888f61",
   "metadata": {},
   "source": [
    "\n",
    "**Autor: Carlos Mir Martínez**\n",
    "\n",
    "**Fecha: 15/06/2022**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
